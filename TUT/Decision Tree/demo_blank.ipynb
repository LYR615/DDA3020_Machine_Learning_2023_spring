{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76f18c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ea747c",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "---\n",
    "For today's tutorial, I will show the demos of tree-base methods.\n",
    "While taking this tutorial, you can run this notebook step by step.\n",
    "I will use the python package called *scikit-learn* for models and *pandas* for feature visualization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0f3b8f",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "---\n",
    "In this section, I will show the demo of Decision Tree (DT) to classify the hand-written digits.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "- Data Preparation\n",
    "- Data Visualization\n",
    "- Sample Codes\n",
    "    - Single Tree\n",
    "    - Bagging Classification\n",
    "    - Random Forest\n",
    "- Feature Importance\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "- [Decision Trees](https://scikit-learn.org/stable/modules/tree.html)\n",
    "- [Bagging Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)\n",
    "- [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd52ba1",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "---\n",
    "\n",
    "In this demo, I will use MNIST dataset, which includes the hand-written digits and is mostly used for classification.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a4fd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml(\"mnist_784\", data_home=\"mnist/\")\n",
    "data_num = 3000\n",
    "X = mnist.data.values[:data_num]/255\n",
    "y = np.array(mnist.target.values[:data_num], dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a7180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.sort(pd.unique(y))\n",
    "n_samples = X.shape[0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "print(f\"# of training data : {len(y_train)}\")\n",
    "print(f\"# of test data     : {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fc0ecf",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "---\n",
    "Since data is originally represented as 2d image, we can visualize it with the reshaping function.\n",
    "Since some codes are unique to python (numpy) programming, you do not need to obsess with this cell.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a778841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Adjustable Parameters ---\n",
    "\n",
    "display_num = 3\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "width = len(classes)\n",
    "length = display_num\n",
    "fig = plt.figure(figsize=(width*3, length*3.5))\n",
    "for i in classes:\n",
    "    images = X_train[y_train==i][:display_num]\n",
    "    for j in range(display_num):\n",
    "        ax = fig.add_subplot(length, width, j*width+i+1)\n",
    "        ax.set_axis_off()\n",
    "        image = images[j].reshape(28, 28)\n",
    "        ax.imshow(image, cmap=plt.cm.gray_r)\n",
    "        ax.set_title(f\"Train: {i}\", fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67057b50",
   "metadata": {},
   "source": [
    "## Coding Samples\n",
    "---\n",
    "Let's train your own tree models to classify hand-written digits.\n",
    "Among the models explained in the lesson (please check *DDA3020 Lecture 08 Decision Tree*), I will introduce the demos of Single Tree, Bagging Classifier, and Random Forest.\n",
    "And, since they have tree-related hyperparameters to set, I will conduct a hyper-parameter tuning later in this tutorial.\n",
    "\n",
    "---\n",
    "\n",
    "### Single Tree\n",
    "---\n",
    "Among hyper-parameters, I will focus on two hyper-parameters: `max_depth` and `min_samples_split` (ppg.53 in the slide).\n",
    "\n",
    "- Functions\n",
    "    - `DecisionTreeClassifier`: Define Single Tree.\n",
    "    \n",
    "\n",
    "- Hyper-Parameters\n",
    "    - `max_depth`: The maximum depth of the tree\n",
    "    - `min_samples_split`: The minimum number of samples required to split an internal node\n",
    "    \n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d7742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Adjustable Parameters --- #\n",
    "\n",
    "max_depth = 8\n",
    "min_samples_split = 2\n",
    "\n",
    "# ----------------------------- #\n",
    "\n",
    "# --- Add Your Codes Here --- #\n",
    "\n",
    "# --------------------------- #\n",
    "print(f\"Train Accuracy : {clf.score(X_train, y_train)}\")\n",
    "print(f\"Test  Accuracy : {clf.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a43aac4",
   "metadata": {},
   "source": [
    "### Bagging Classifier\n",
    "---\n",
    "As explained in the lecture, Bagging Classifier includes multiple classifiers with different sub-training dataset.\n",
    "We have `n_estimators` to set the \\# of classifiers (decision trees).\n",
    "\n",
    "Note: We can use any classifier for Bagging Classifier (ex. SVC)\n",
    "\n",
    "- Functions\n",
    "    - `BaggingClassifier`: Define Bagging Classifier\n",
    "    \n",
    "\n",
    "- Hyper-Parameters\n",
    "    - `n_estimators`: \\# of classifiers\n",
    "    \n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b288d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Adjustable Parameters --- #\n",
    "\n",
    "max_depth = 8\n",
    "min_samples_split = 2\n",
    "n_estimators = 10\n",
    "\n",
    "# ----------------------------- #\n",
    "\n",
    "# --- Add Your Codes Here --- #\n",
    "\n",
    "# --------------------------- #\n",
    "print(f\"Train Accuracy : {clf.score(X_train, y_train)}\")\n",
    "print(f\"Test  Accuracy : {clf.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f885cb",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "---\n",
    "\n",
    "For Random Forest, it has the same hyper-parameters as Bagging Classifier but expected to be better than that.\n",
    "\n",
    "- Functions\n",
    "    - `RandomForestClassifier`: Define Random Forest\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c178687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Adjustable Parameters --- #\n",
    "\n",
    "max_depth = 8\n",
    "min_samples_split = 2\n",
    "n_estimators = 10\n",
    "\n",
    "# ----------------------------- #\n",
    "\n",
    "# --- Add Your Codes Here --- #\n",
    "\n",
    "# --------------------------- #\n",
    "print(f\"Train Accuracy : {clf.score(X_train, y_train)}\")\n",
    "print(f\"Test  Accuracy : {clf.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571c804d",
   "metadata": {},
   "source": [
    "## Hyper-Parameters Tuning\n",
    "---\n",
    "\n",
    "As explained in the lesson, tree-based models are vulnerable to overfitting.\n",
    "So, we need to find better hyperparameters (high accuracy and robust to overfitting) via hyper-parameters tuning.\n",
    "The most-known tuning method is Grid Search.\n",
    "It generates candidates from a grid of parameter values you specify and find the hyper-parameters with the best result.\n",
    "In this demo, I will use the more efficient one which is called Randomized Search.\n",
    "It randomly generates candidates from the chosen range.\n",
    "\n",
    "`cv` in arguments of `RandomizedSearchCV` indicates the $k$ in [Cross Validation](https://scikit-learn.org/stable/modules/cross_validation.html).\n",
    "\n",
    "- Hyper-Parameters\n",
    "    - `max_depth`: The maximum depth of the tree\n",
    "    - `min_samples_split`: The minimum number of samples required to split an internal node\n",
    "    \n",
    "---\n",
    "### Single Tree\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6b18a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Range for Randomized Search\n",
    "\n",
    "# --- Add Your Codes Here --- #\n",
    "\n",
    "# --------------------------- #\n",
    "\n",
    "# Random Search\n",
    "\n",
    "# --- Add Your Codes Here --- #\n",
    "\n",
    "# --------------------------- #\n",
    "\n",
    "best_params = search.best_params_\n",
    "print(\"Best Parameters:\")\n",
    "for key in best_params:\n",
    "    print(f\"    {key}: {best_params[key]}\")\n",
    "    \n",
    "# Classification with Best Parameters\n",
    "    \n",
    "# --- Add Your Codes Here --- #\n",
    "\n",
    "# --------------------------- #\n",
    "\n",
    "models[\"single tree\"] = st\n",
    "print()\n",
    "print(f\"Train Accuracy : {st.score(X_train, y_train)}\")\n",
    "print(f\"Test  Accuracy : {st.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acde7d5d",
   "metadata": {},
   "source": [
    "### Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8cc8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "bc = BaggingClassifier(base_estimator=dt,\n",
    "                        n_estimators=10,\n",
    "                        random_state=0)\n",
    "\n",
    "# --- Edit Your Codes Here --- #\n",
    "distributions = dict(\n",
    "    max_depth=np.arange(5, 31),\n",
    "    min_samples_split=2**np.arange(0, 4),\n",
    ")\n",
    "# ---------------------------- #\n",
    "\n",
    "# Random Search\n",
    "\n",
    "clf = RandomizedSearchCV(bc, distributions, random_state=0, n_iter=20, cv=3, n_jobs=-1)\n",
    "search = clf.fit(X_train, y_train)\n",
    "best_params = search.best_params_\n",
    "print(\"Best Parameters:\")\n",
    "for key in best_params:\n",
    "    print(f\"    {key}: {best_params[key]}\")\n",
    "    \n",
    "# Classification with Best Parameters\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=0,\n",
    "                             min_samples_split=best_params[\"base_estimator__min_samples_split\"],\n",
    "                             max_depth=best_params[\"base_estimator__max_depth\"])\n",
    "\n",
    "bc = BaggingClassifier(base_estimator=dt,\n",
    "                        n_estimators=10,\n",
    "                        random_state=0)\n",
    "\n",
    "bc.fit(X_train, y_train)\n",
    "models[\"bagging\"] = bc\n",
    "\n",
    "print()\n",
    "print(f\"Train Accuracy : {bc.score(X_train, y_train)}\")\n",
    "print(f\"Test  Accuracy : {bc.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea583f9",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7d6466",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=0,\n",
    "                             n_estimators=10)\n",
    "distributions = dict(\n",
    "    max_depth=np.arange(5, 31),\n",
    "    min_samples_split=2**np.arange(0, 4),\n",
    ")\n",
    "\n",
    "# Random Search\n",
    "\n",
    "clf = RandomizedSearchCV(rf, distributions, random_state=0, n_iter=20, cv=3, n_jobs=-1)\n",
    "search = clf.fit(X_train, y_train)\n",
    "best_params = search.best_params_\n",
    "print(\"Best Parameters:\")\n",
    "for key in best_params:\n",
    "    print(f\"    {key}: {best_params[key]}\")\n",
    "    \n",
    "# Classification with Best Parameters\n",
    "    \n",
    "rf = RandomForestClassifier(random_state=0,\n",
    "                            n_estimators=10,\n",
    "                             min_samples_split=best_params[\"min_samples_split\"],\n",
    "                             max_depth=best_params[\"max_depth\"]\n",
    "                            )\n",
    "rf.fit(X_train, y_train)\n",
    "models[\"random forest\"] = rf\n",
    "\n",
    "print()\n",
    "print(f\"Train Accuracy : {rf.score(X_train, y_train)}\")\n",
    "print(f\"Test  Accuracy : {rf.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f3cac8",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "---\n",
    "One of other advantages of using tree-based models is its interpretability.\n",
    "In other words, We can easily understand how the models process data.\n",
    "Among them, I will introduce feature importance, which shows how each feature (in our case, pixels of images) affects the result.\n",
    "In tree-based models, it indicates how many times each feature was used to split the internal node.\n",
    "\n",
    "- New Attributes\n",
    "    - `feature_importances_`: feature importance of each feature\n",
    "    \n",
    "---\n",
    "### Single Tree\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da85a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[\"single tree\"]\n",
    "\n",
    "fi = model.feature_importances_\n",
    "fi_df = pd.DataFrame([np.array([f\"pixel-({i//28}, {i%28})\" for i in range(784)]), fi], index=[\"name\", \"importance\"]).T\n",
    "fi_df[\"importance\"] = fi_df[\"importance\"].values.astype(\"float\")\n",
    "\n",
    "fi_df.sort_values(by=\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f004644c",
   "metadata": {},
   "source": [
    "### Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a206fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[\"bagging\"]\n",
    "\n",
    "fi = np.zeros((784))\n",
    "for tree in model.estimators_:\n",
    "    fi += tree.feature_importances_\n",
    "fi = fi/len(model.estimators_)\n",
    "fi_df = pd.DataFrame([np.array([f\"pixel-({i//28}, {i%28})\" for i in range(784)]), fi], index=[\"name\", \"importance\"]).T\n",
    "fi_df[\"importance\"] = fi_df[\"importance\"].values.astype(\"float\")\n",
    "\n",
    "fi_df.sort_values(by=\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cea3f62",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bcd667",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[\"random forest\"]\n",
    "\n",
    "fi = model.feature_importances_\n",
    "fi_df = pd.DataFrame([np.array([f\"pixel-({i//28}, {i%28})\" for i in range(784)]), fi], index=[\"name\", \"importance\"]).T\n",
    "fi_df[\"importance\"] = fi_df[\"importance\"].values.astype(\"float\")\n",
    "\n",
    "fi_df.sort_values(by=\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd7a99",
   "metadata": {},
   "source": [
    "### 2d Representation of Feature Importance\n",
    "---\n",
    "\n",
    "Of course, the feature name of MNIST is not intuitive, so let's visualize it in 2d-representation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183e1deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = len(models)\n",
    "length = 1\n",
    "\n",
    "fig = plt.figure(figsize=(width*5, length*5.5))\n",
    "\n",
    "for i, name in enumerate(models):\n",
    "    if name in [\"bagging\"]:\n",
    "        fi = np.zeros((784))\n",
    "        for tree in models[name].estimators_:\n",
    "            fi += tree.feature_importances_\n",
    "        fi = fi/len(models[name].estimators_)\n",
    "    else:\n",
    "        fi = models[name].feature_importances_\n",
    "    ax = fig.add_subplot(length, width, i+1)\n",
    "    ax.imshow(fi.reshape(28, 28))\n",
    "    ax.set_title(name, fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dbbc8f",
   "metadata": {},
   "source": [
    "### Let's Observe the Validity of Feature Importance\n",
    "---\n",
    "Let's modify the values of the *un-important* features and the *important* features; and see the result.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a165816",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = np.zeros((28, 28))\n",
    "frame[[0,-1],:] = 1\n",
    "frame[:, [0,-1]] = 1\n",
    "X_test_edge = X_test + frame.reshape(1, 784)\n",
    "X_test_edge[X_test_edge>1] = 1\n",
    "\n",
    "width = len(classes)\n",
    "length = 2\n",
    "fig = plt.figure(figsize=(width*3, length*3.5))\n",
    "for i in classes:\n",
    "    images = X_test[y_test==i][:1]\n",
    "    ax = fig.add_subplot(length, width, i+1)\n",
    "    ax.set_axis_off()\n",
    "    image = images[0].reshape(28, 28)\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r)\n",
    "    ax.set_title(f\"Test: {i}\", fontsize=15)\n",
    "    \n",
    "    images = X_test_edge[y_test==i][:1]\n",
    "    ax = fig.add_subplot(length, width, width+i+1)\n",
    "    ax.set_axis_off()\n",
    "    image = images[0].reshape(28, 28)\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r)\n",
    "    ax.set_title(f\"Test(Edge): {i}\", fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bc3d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = np.zeros((28, 28))\n",
    "frame = np.zeros((28, 28))\n",
    "frame[11:17,11:17] = 1\n",
    "X_test_center = X_test + frame.reshape(1, 784)\n",
    "X_test_center[X_test_center>1] = 1\n",
    "\n",
    "width = len(classes)\n",
    "length = 2\n",
    "fig = plt.figure(figsize=(width*3, length*3.5))\n",
    "for i in classes:\n",
    "    images = X_test[y_test==i][:1]\n",
    "    ax = fig.add_subplot(length, width, i+1)\n",
    "    ax.set_axis_off()\n",
    "    image = images[0].reshape(28, 28)\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r)\n",
    "    ax.set_title(f\"Test: {i}\", fontsize=15)\n",
    "    \n",
    "    images = X_test_center[y_test==i][:1]\n",
    "    ax = fig.add_subplot(length, width, width+i+1)\n",
    "    ax.set_axis_off()\n",
    "    image = images[0].reshape(28, 28)\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r)\n",
    "    ax.set_title(f\"Test(Center): {i}\", fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d8fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(models):\n",
    "    print(f\"Classifier: {name}\")\n",
    "    print(f\"    Original Test Accuracy: {models[name].score(X_test, y_test)}\")\n",
    "    print(f\"    Edge     Test Accuracy: {models[name].score(X_test_edge, y_test)}\")\n",
    "    print(f\"    Center   Test Accuracy: {models[name].score(X_test_center, y_test)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89acee6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
